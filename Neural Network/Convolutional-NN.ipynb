{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a5af1e-5171-4efe-9ca9-ea5c23993f47",
   "metadata": {},
   "source": [
    "# Convolutional Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e02e3326-cab1-4150-bf92-760d678925cf",
   "metadata": {},
   "source": [
    "We will be working with the cifar-10 dataset. This dataset has images from 10 different classes with images that are 32x32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d8033d-7531-4645-bc57-30d3cdc93b72",
   "metadata": {},
   "source": [
    "The images belong to the following classes: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43307294-4e76-4c04-a48d-f915a92e61cd",
   "metadata": {},
   "source": [
    "<li> airplane\n",
    "<li> automobile\n",
    "<li> bird\n",
    "<li>  cat\n",
    "<li> deer\n",
    "<li> dog\n",
    "<li>  frog\n",
    "<li>  horse\n",
    "<li>  ship\n",
    "<li>  truck"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e883b3-4f54-43eb-be3d-91ccd8bac92d",
   "metadata": {},
   "source": [
    "Here is a link to the dataset: https://www.cs.toronto.edu/~kriz/cifar.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c5fb9a-37fa-446a-a496-609a1eaa5875",
   "metadata": {},
   "source": [
    "#### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73c03a3a-29d3-4e9a-a932-2e897f5e5fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.datasets import cifar10\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a36ec8b1-2d2e-4c3a-8523-cd7a41500bc9",
   "metadata": {},
   "source": [
    "#### Loading the data, but shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fdf3bcbe-3eae-4a2f-a96e-1f88c107af33",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab67f19c-b3fc-469a-9317-c78ce279d4b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ec49734e-3e85-4f07-b024-4d76b0adf102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32, 3)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5d4947-d00e-4959-8916-1592146b1c12",
   "metadata": {},
   "source": [
    "***The 32 by 32 by 3 is the color images***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6644ff0b-ce25-43cd-9784-ca9f6405ee1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000 total images for the training data\n",
      "10000 total images for the test data\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape[0], 'total images for the training data')\n",
    "print(X_test.shape[0], 'total images for the test data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b31aa0-788a-4d8b-ba77-6a7e30601a0a",
   "metadata": {},
   "source": [
    "#### Printing a Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d93c9eb-e683-48bd-9eb6-ef7b972baa57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7ff06b43d100>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(y_train[500])\n",
    "plt.imshow(X_train[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18715242-a261-4736-84ff-25dbb5d4469a",
   "metadata": {},
   "source": [
    "***This is a dog image, although hard to see***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a84781d-1963-4583-860f-d7e6036ae0e0",
   "metadata": {},
   "source": [
    "#### Encoding the data (OneHotEncoding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "332bd125-db0c-4474-8327-8b14e4e3cdfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = 10\n",
    "y_train = keras.utils.to_categorical(y_train, classes) \n",
    "y_test = keras.utils.to_categorical(y_test, classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b60ed0b0-271a-4387-a132-05bfeafa7c6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(y_train[500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7061ea88-a91a-4221-a276-4b86a442206f",
   "metadata": {},
   "source": [
    "***If you noticed, y_test is now a vector of 0 and 1 instead of a number between 0-9. This will help us when we are using our Neural Network***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9a65c3-c69b-4c1e-a1f3-b643d829c28e",
   "metadata": {},
   "source": [
    "#### Scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53250a8e-4098-4bff-847d-35c3a103a63d",
   "metadata": {},
   "source": [
    "***We need to scale all values so that they are between 0 and 1 because our neural networks require inputs that are scaled. The\n",
    "best way to do this, is to divide each value in X_train by 255 since all rgb values range from 0-255. This will ensure all of our values\n",
    "are between 0 and 1***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dce6a4fe-4b46-4087-89a1-25134ef3baca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 59,  62,  63],\n",
       "        [ 43,  46,  45],\n",
       "        [ 50,  48,  43],\n",
       "        ...,\n",
       "        [158, 132, 108],\n",
       "        [152, 125, 102],\n",
       "        [148, 124, 103]],\n",
       "\n",
       "       [[ 16,  20,  20],\n",
       "        [  0,   0,   0],\n",
       "        [ 18,   8,   0],\n",
       "        ...,\n",
       "        [123,  88,  55],\n",
       "        [119,  83,  50],\n",
       "        [122,  87,  57]],\n",
       "\n",
       "       [[ 25,  24,  21],\n",
       "        [ 16,   7,   0],\n",
       "        [ 49,  27,   8],\n",
       "        ...,\n",
       "        [118,  84,  50],\n",
       "        [120,  84,  50],\n",
       "        [109,  73,  42]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[208, 170,  96],\n",
       "        [201, 153,  34],\n",
       "        [198, 161,  26],\n",
       "        ...,\n",
       "        [160, 133,  70],\n",
       "        [ 56,  31,   7],\n",
       "        [ 53,  34,  20]],\n",
       "\n",
       "       [[180, 139,  96],\n",
       "        [173, 123,  42],\n",
       "        [186, 144,  30],\n",
       "        ...,\n",
       "        [184, 148,  94],\n",
       "        [ 97,  62,  34],\n",
       "        [ 83,  53,  34]],\n",
       "\n",
       "       [[177, 144, 116],\n",
       "        [168, 129,  94],\n",
       "        [179, 142,  87],\n",
       "        ...,\n",
       "        [216, 184, 140],\n",
       "        [151, 118,  84],\n",
       "        [123,  92,  72]]], dtype=uint8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c94e2c61-84eb-48f5-ae11-d5b6a7209a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed7a1e73-f274-4779-a78e-e8e35e75fa81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.23137255, 0.24313726, 0.24705882],\n",
       "        [0.16862746, 0.18039216, 0.1764706 ],\n",
       "        [0.19607843, 0.1882353 , 0.16862746],\n",
       "        ...,\n",
       "        [0.61960787, 0.5176471 , 0.42352942],\n",
       "        [0.59607846, 0.49019608, 0.4       ],\n",
       "        [0.5803922 , 0.4862745 , 0.40392157]],\n",
       "\n",
       "       [[0.0627451 , 0.07843138, 0.07843138],\n",
       "        [0.        , 0.        , 0.        ],\n",
       "        [0.07058824, 0.03137255, 0.        ],\n",
       "        ...,\n",
       "        [0.48235294, 0.34509805, 0.21568628],\n",
       "        [0.46666667, 0.3254902 , 0.19607843],\n",
       "        [0.47843137, 0.34117648, 0.22352941]],\n",
       "\n",
       "       [[0.09803922, 0.09411765, 0.08235294],\n",
       "        [0.0627451 , 0.02745098, 0.        ],\n",
       "        [0.19215687, 0.10588235, 0.03137255],\n",
       "        ...,\n",
       "        [0.4627451 , 0.32941177, 0.19607843],\n",
       "        [0.47058824, 0.32941177, 0.19607843],\n",
       "        [0.42745098, 0.28627452, 0.16470589]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.8156863 , 0.6666667 , 0.3764706 ],\n",
       "        [0.7882353 , 0.6       , 0.13333334],\n",
       "        [0.7764706 , 0.6313726 , 0.10196079],\n",
       "        ...,\n",
       "        [0.627451  , 0.52156866, 0.27450982],\n",
       "        [0.21960784, 0.12156863, 0.02745098],\n",
       "        [0.20784314, 0.13333334, 0.07843138]],\n",
       "\n",
       "       [[0.7058824 , 0.54509807, 0.3764706 ],\n",
       "        [0.6784314 , 0.48235294, 0.16470589],\n",
       "        [0.7294118 , 0.5647059 , 0.11764706],\n",
       "        ...,\n",
       "        [0.72156864, 0.5803922 , 0.36862746],\n",
       "        [0.38039216, 0.24313726, 0.13333334],\n",
       "        [0.3254902 , 0.20784314, 0.13333334]],\n",
       "\n",
       "       [[0.69411767, 0.5647059 , 0.45490196],\n",
       "        [0.65882355, 0.5058824 , 0.36862746],\n",
       "        [0.7019608 , 0.5568628 , 0.34117648],\n",
       "        ...,\n",
       "        [0.84705883, 0.72156864, 0.54901963],\n",
       "        [0.5921569 , 0.4627451 , 0.32941177],\n",
       "        [0.48235294, 0.36078432, 0.28235295]]], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d08caa-7add-423a-9039-33037e262c3e",
   "metadata": {},
   "source": [
    "#### CNN Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719611e6-e43c-4a31-9229-971929dcf2a3",
   "metadata": {},
   "source": [
    "There are three layers to the CNN which I will add. This will be a convolutional layer, MaxPooling, and then flattening the image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0068f9ef-36d8-40ae-a0ad-caa3ace8d2e9",
   "metadata": {},
   "source": [
    "#### 1. Convolutional: \n",
    "#### 2. MaxPooling: \n",
    "#### 3. Flattening: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bedbb0f-1f4f-475c-b8db-f7c25cd27df1",
   "metadata": {},
   "source": [
    "#### Building a CNN with: a 5 by 5 convolution, 2 by 2 stride, and 32 filters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9d5e04-2af2-4630-8897-36b304f832ae",
   "metadata": {},
   "source": [
    "***Adding our first 5X5 convolution layer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5328b580-a906-4002-beac-c11379dc0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (5, 5), strides = (2,2), padding='same',input_shape=X_train.shape[1:]))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd070209-a9ff-4a43-a1f5-17aba769054c",
   "metadata": {},
   "source": [
    "***Adding our second 5X5 convolution layer***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fe164446-ad5a-4256-ba3e-bfc8e6d1df4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(32, (5, 5), strides = (2,2)))\n",
    "model.add(Activation('relu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32331df1-ad2a-40b9-bcef-5cc31e6f396a",
   "metadata": {},
   "source": [
    "***Applying Max pooling and a droput for regularization***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b260bfce-4cd0-4052-b41c-dd248a1fdc0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bfd3e12-14bf-4aa0-991d-cf4d16ca17d5",
   "metadata": {},
   "source": [
    "***Flattening the image to make it one dimensional, then adding more dropout, then finally a activation neuron to classifiy the image***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eda1c5c-1779-4ffd-a9f7-6d5ccff90463",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten()) \n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(classes))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d26ec37-5d66-4806-8b18-11c746054e49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        2432      \n",
      "                                                                 \n",
      " activation (Activation)     (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 6, 6, 32)          25632     \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 6, 6, 32)          0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 3, 3, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 3, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 288)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 512)               147968    \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 181,162\n",
      "Trainable params: 181,162\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08341087-b9ec-4a3c-8b6b-0b5ebc79f9e9",
   "metadata": {},
   "source": [
    "#### Compiling the Neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac7ad9f0-d115-4844-b2af-124ce38a6e0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "opt = keras.optimizers.RMSprop(learning_rate=0.0005, decay=1e-6)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer=opt,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c231842-577c-4946-9170-2a085832b13e",
   "metadata": {},
   "source": [
    "#### Fitting the model (Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "429ae6a6-914c-49e4-a930-f46f25661f18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1563/1563 [==============================] - 17s 10ms/step - loss: 1.7296 - accuracy: 0.3689 - val_loss: 1.4874 - val_accuracy: 0.4461\n",
      "Epoch 2/2\n",
      "1563/1563 [==============================] - 20s 13ms/step - loss: 1.4498 - accuracy: 0.4765 - val_loss: 1.5316 - val_accuracy: 0.4747\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff05a713400>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,batch_size=batch_size,epochs=2,validation_data=(X_test, y_test),shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e5362edf-a6a1-4e12-8f69-8a45cbaeb530",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predict_x=model.predict(X_test) \n",
    "classes_x=np.argmax(predict_x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4cd9a85c-1055-44db-bdc6-2543e0afe176",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4747"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test, axis=1)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy_score(np.argmax(y_test, axis=1), classes_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ccfb81b-6c42-4ba1-97c6-52ca14b0e395",
   "metadata": {},
   "source": [
    "#### Lets now build a more complex model and see if we can improve our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a184990a-e601-410c-9465-2d1e6b71ee7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce309579-9b16-4e56-92c9-fe672b219669",
   "metadata": {},
   "source": [
    "#### Creating out First layer convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "28f0a970-6be3-440b-b490-a6d0d5adc361",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(Conv2D(32, (3, 3), padding='same',input_shape=X_train.shape[1:]))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv2D(32, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61a2998-564e-40b0-9e81-b913fccb7e64",
   "metadata": {},
   "source": [
    "#### Creating out Second layer convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fbf40a6e-a72c-4d65-bdda-ddd3c63ce646",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Conv2D(64, (3, 3)))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model2.add(Dropout(0.25))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac9002d-6b38-4346-9057-92b0904c3cad",
   "metadata": {},
   "source": [
    "#### Flattening then fully connecting the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e2153247-a626-4cb6-a512-182c4c315022",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.add(Flatten())\n",
    "model2.add(Dense(512))\n",
    "model2.add(Activation('relu'))\n",
    "model2.add(Dropout(0.5))\n",
    "model2.add(Dense(classes))\n",
    "model2.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "321f6489-bf7b-42f0-b196-d464c88dec6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_2 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 30, 30, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 13, 13, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 2304)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 512)               1180160   \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d10b8-74b6-4c01-8b19-0cb8ba121248",
   "metadata": {},
   "source": [
    "#### Compiling the neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "25bfb991-3f04-4ca2-b5a0-58d7b11e7d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "opt2 = keras.optimizers.RMSprop(learning_rate=0.0005)\n",
    "model2.compile(loss='categorical_crossentropy',optimizer=opt2,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27e5e48-3ae3-416a-9f27-ec02be4b1971",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ab9fdcc7-9a3b-454a-b398-780d12d00068",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1563/1563 [==============================] - 144s 92ms/step - loss: 1.5588 - accuracy: 0.4351 - val_loss: 1.2424 - val_accuracy: 0.5612\n",
      "Epoch 2/2\n",
      "1563/1563 [==============================] - 117s 75ms/step - loss: 1.1456 - accuracy: 0.5972 - val_loss: 1.0507 - val_accuracy: 0.6378\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7ff05a90fb80>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size=32\n",
    "model2.fit(X_train, y_train,batch_size=batch_size,epochs=2,validation_data=(X_test, y_test),shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5a8e83-a413-4057-8544-206a1b897ab6",
   "metadata": {},
   "source": [
    "#### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd27bb3b-dc64-436d-a03c-1ddb44a4a89d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_x=model2.predict(X_test) \n",
    "classes_x=np.argmax(predict_x,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e08a4bf-1b10-4ab2-a231-961990d47e1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6378"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_test, axis=1)\n",
    "accuracy_score(np.argmax(y_test, axis=1), classes_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feab2f01-e074-4656-9074-034c4f640621",
   "metadata": {},
   "source": [
    "#### Lets make a third model, but this time very complex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "71cb9eeb-4dbf-4148-91b3-8a5ed356c1b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 32, 32, 32)        896       \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 32, 32, 32)        0         \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 30, 30, 32)        9248      \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 30, 30, 32)        0         \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 15, 15, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 15, 15, 32)        0         \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 15, 15, 64)        18496     \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 15, 15, 64)        0         \n",
      "                                                                 \n",
      " conv2d_9 (Conv2D)           (None, 13, 13, 64)        36928     \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 13, 13, 64)        0         \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 6, 6, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " conv2d_10 (Conv2D)          (None, 6, 6, 64)          36928     \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 6, 6, 64)          0         \n",
      "                                                                 \n",
      " conv2d_11 (Conv2D)          (None, 4, 4, 64)          36928     \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 4, 4, 64)          0         \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 2, 2, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 2, 2, 64)          0         \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 10)                5130      \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 276,138\n",
      "Trainable params: 276,138\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model3 = Sequential()\n",
    "model3.add(Conv2D(32, (3, 3), padding='same',input_shape=X_train.shape[1:]))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Conv2D(32, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Conv2D(64, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Conv2D(64, (3, 3)))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model3.add(Dropout(0.25))\n",
    "\n",
    "model3.add(Flatten())\n",
    "model3.add(Dense(512))\n",
    "model3.add(Activation('relu'))\n",
    "model3.add(Dropout(0.5))\n",
    "model3.add(Dense(classes))\n",
    "model3.add(Activation('softmax'))\n",
    "\n",
    "model3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2a1a514d-5ea9-45ca-a35d-09950f813f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "1563/1563 [==============================] - 124s 78ms/step - loss: 1.7123 - accuracy: 0.3644 - val_loss: 1.5705 - val_accuracy: 0.4516\n",
      "Epoch 2/2\n",
      "1563/1563 [==============================] - 133s 85ms/step - loss: 1.3161 - accuracy: 0.5265 - val_loss: 1.1040 - val_accuracy: 0.6053\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fefe3a84130>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt3 = keras.optimizers.RMSprop(learning_rate=0.0005)\n",
    "model3.compile(loss='categorical_crossentropy',optimizer=opt3,metrics=['accuracy'])\n",
    "\n",
    "batch_size=32\n",
    "model3.fit(X_train, y_train,batch_size=batch_size,epochs=2,validation_data=(X_test, y_test),shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "30186ebb-9c6b-4217-89ed-861fa98fd996",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 4s 12ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6053"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_x=model3.predict(X_test) \n",
    "classes_x=np.argmax(predict_x,axis=1)\n",
    "\n",
    "np.argmax(y_test, axis=1)\n",
    "accuracy_score(np.argmax(y_test, axis=1), classes_x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124e0121-99da-4d15-a628-b288c219eaeb",
   "metadata": {},
   "source": [
    "#### Summary: We got the best performance when we had only two layers for our CNN. From here, we could hypertune even more if we wanted by adjusting the dropout rate, activation, strides, pooling, etc. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
